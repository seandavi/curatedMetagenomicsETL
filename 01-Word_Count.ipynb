{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020 Google Inc.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
    "<!--\n",
    "    Licensed to the Apache Software Foundation (ASF) under one\n",
    "    or more contributor license agreements.  See the NOTICE file\n",
    "    distributed with this work for additional information\n",
    "    regarding copyright ownership.  The ASF licenses this file\n",
    "    to you under the Apache License, Version 2.0 (the\n",
    "    \"License\"); you may not use this file except in compliance\n",
    "    with the License.  You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "    Unless required by applicable law or agreed to in writing,\n",
    "    software distributed under the License is distributed on an\n",
    "    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "    KIND, either express or implied.  See the License for the\n",
    "    specific language governing permissions and limitations\n",
    "    under the License.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Word Count\n",
    "\n",
    "This example demonstrates how to set up an Apache Beam pipeline that reads from a\n",
    "[Google Cloud Storage](https://cloud.google.com/storage) file containing text from Shakespeare's work *King Lear*, \n",
    "tokenizes the text lines into individual words, and performs a frequency count on each of those words. \n",
    "\n",
    "An Apache Beam pipeline is a pipeline that reads input data, transforms that\n",
    "data, and writes output data. It consists of `PTransform`s and `PCollection`s.\n",
    "A `PCollection` represents a distributed data set that your Beam pipeline operates on.\n",
    "A `PTransform` represents a data processing operation, or a step, in your pipeline.\n",
    "It takes one or more `PCollection`s as input, performs a processing function\n",
    "that you provide on the elements of that `PCollection`, and produces zero\n",
    "or more output `PCollection` objects.\n",
    "\n",
    "For details about Apache Beam pipelines, including `PTransform`s and\n",
    "`PCollection`s, visit [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/).\n",
    "\n",
    "You'll be able to use this notebook to explore the data in each `PCollection`.\n",
    "\n",
    "We first start with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's regular expression library\n",
    "import re\n",
    "\n",
    "# Beam and interactive Beam imports\n",
    "import apache_beam.io.fileio\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following defines a `PTransform` named `ReadWordsFromText`, that extracts words from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadWordsFromText(beam.PTransform):\n",
    "    \n",
    "    def __init__(self, file_pattern):\n",
    "        self._file_pattern = file_pattern\n",
    "    \n",
    "    def expand(self, pcoll):\n",
    "        return (pcoll.pipeline\n",
    "                | beam.io.ReadFromText(self._file_pattern)\n",
    "                | beam.FlatMap(lambda line: re.findall(r'[\\w\\']+', line.strip(), re.UNICODE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up an Apache Beam pipeline with the *Interactive Runner*.\n",
    "The *Interactive Runner* is the runner suitable for running in notebooks.\n",
    "A runner is an execution engine for Apache Beam pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = beam.Pipeline(InteractiveRunner())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up a `PTransform` that extracts words from a Google Cloud Storage file that contains the text of Shakespeare's work *King Lear*.\n",
    "\n",
    "`|` is an overloaded operator that applies a `PTransform` to a `PCollection` to produce a new `PCollection`.\n",
    "Together with `|`, `>>` allows you to optionally name a `PTransform`.\n",
    "\n",
    "Usage: `<PCollection> | <PTransform>` or `<PCollection> | <name> >> <PTransform>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions(\n",
    "    runner='DataflowRunner',\n",
    "    project='curatedmetagenomicdata',\n",
    "    job_name='genefamilies_relab',\n",
    "    temp_location='gs://cmgd-data/temp',\n",
    "    region='us-central1')\n",
    "\n",
    "f = (p \n",
    "     | beam.io.fileio.MatchFiles('gs://cmgd-data/manual/AsnicarF_2017/marker_/MV_FEI1_t1Q14.tsv') \n",
    "     | beam.io.WriteToText('gs://cmgd-data/df/abc'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up a `PTransform` to count the words. `counts` is a `PCollection` that will contain the count data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-3cfe9cee-9715-4fd6-86e8-c44cabe418d8.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-3cfe9cee-9715-4fd6-86e8-c44cabe418d8.json']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import apache_beam as beam\n",
    "import json\n",
    "\n",
    "# Python's regular expression library\n",
    "import re\n",
    "\n",
    "# Beam and interactive Beam imports\n",
    "import apache_beam.io.fileio\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "import logging\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions(\n",
    "    runner='DataflowRunner',\n",
    "    project='curatedmetagenomicdata',\n",
    "    job_name='marker-presence', #no underlines allowed\n",
    "    temp_location='gs://cmgd-data/temp',\n",
    "    staging_location='gs://cmgd-data/temp',\n",
    "    region='us-central1')\n",
    "\n",
    "class CSVFixer(beam.DoFn):\n",
    "    def process(self, fobj):\n",
    "        import apache_beam as beam\n",
    "        res = []\n",
    "        parts = fobj.path.split('/')\n",
    "        study_id=parts[4]\n",
    "        filetype = parts[5]\n",
    "        sample_id=parts[6].replace('.tsv','')\n",
    "        with beam.io.gcp.gcsfilesystem.GCSFileSystem({}).open(fobj.path,'r') as f:\n",
    "            logging.info(study_id,filetype,sample_id)\n",
    "            for r in f:\n",
    "                row = r.decode('UTF-8')\n",
    "                if(row.startswith('#')):\n",
    "                    continue\n",
    "                rowsplit=row.strip().split('\\t')\n",
    "                \n",
    "                # marker_abundance\n",
    "                #d = {\n",
    "                #    'marker': rowsplit[0],\n",
    "                #    'abundance': float(rowsplit[1])\n",
    "                #}  \n",
    "                \n",
    "                # marker_presence\n",
    "                d = {\n",
    "                    'marker': rowsplit[0],\n",
    "                    'presence': float(rowsplit[1])>0 # there is an odd 7.007007007 that shows up here\n",
    "                }  \n",
    "                \n",
    "                # pathabundance_relab\n",
    "                #namesplit = rowsplit[0].split('|')\n",
    "                #d = {\n",
    "                #    'pathway': namesplit[0],\n",
    "                #    'unclassified': len(namesplit)>1,\n",
    "                #    'relative_abundance': float(rowsplit[1])\n",
    "                #}\n",
    "                \n",
    "                #pathcoverage \n",
    "                #namesplit = rowsplit[0].split('|')\n",
    "                #d = {\n",
    "                #    'pathway': namesplit[0],\n",
    "                #    'unclassified': len(namesplit)>1,\n",
    "                #    'covered': bool(int(rowsplit[1]))\n",
    "                #}\n",
    "                \n",
    "                # genfamilies_relab\n",
    "                #d = {\n",
    "                #    'taxon': rowsplit[0].split(b'|')[-1],\n",
    "                #    'relative_abundance': float(rowsplit[1]) #,\n",
    "                #    #'ncbi_taxonomy': rowsplit[1].split(b'|')[-1]\n",
    "                #}\n",
    "                d.update({\n",
    "                    'study_id':study_id,\n",
    "                    'sample_id':sample_id,\n",
    "                    'filetype': filetype\n",
    "                })\n",
    "                res.append(d)\n",
    "        return res\n",
    "    \n",
    "\n",
    "\n",
    "#p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "gcs = beam.io.gcp.gcsfilesystem.GCSFileSystem({})\n",
    "#var = 'genefamilies_relab'\n",
    "#var = 'pathabundance_relab'\n",
    "#var = 'pathcoverage'\n",
    "#var = 'marker_abundance'\n",
    "ver = 'marker_presence'\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    (p | beam.io.fileio.MatchFiles(f'gs://cmgd-data/manual/**/{var}/*')\n",
    "     | beam.Reshuffle()\n",
    "     | beam.ParDo(CSVFixer())\n",
    "     | beam.Map(lambda x: json.dumps(x))\n",
    "     | beam.io.WriteToText(f'gs://cmgd-data/temp/{var}.json')\n",
    "    )\n",
    "#p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-3cfe9cee-9715-4fd6-86e8-c44cabe418d8.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-3cfe9cee-9715-4fd6-86e8-c44cabe418d8.json']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-3bb4c1af2103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m      \u001b[0;34m|\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSVFixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m      \u001b[0;34m|\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m      \u001b[0;34m|\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteToText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://cmgd-data/temp/metaphlan_bugs_list.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[0;31m#p.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.29.0/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    581\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.29.0/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1609\u001b[0m       \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m       \u001b[0;31m# TODO: Merge the termination code in poll_for_job_completion and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import apache_beam as beam\n",
    "import json\n",
    "\n",
    "# Python's regular expression library\n",
    "import re\n",
    "\n",
    "# Beam and interactive Beam imports\n",
    "import apache_beam.io.fileio\n",
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "\n",
    "import logging\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "options = PipelineOptions(\n",
    "    runner='DataflowRunner',\n",
    "    project='curatedmetagenomicdata',\n",
    "    job_name='metaphlan-bugs-list',\n",
    "    temp_location='gs://cmgd-data/temp',\n",
    "    staging_location='gs://cmgd-data/temp',\n",
    "    region='us-central1')\n",
    "\n",
    "class CSVFixer(beam.DoFn):\n",
    "    def process(self, fobj):\n",
    "        import apache_beam as beam\n",
    "        res = []\n",
    "        parts = fobj.path.split('/')\n",
    "        study_id=parts[4]\n",
    "        filetype = parts[5]\n",
    "        sample_id=parts[6].replace('.tsv','')\n",
    "        with beam.io.gcp.gcsfilesystem.GCSFileSystem({}).open(fobj.path,'r') as f:\n",
    "            logging.info(study_id,filetype,sample_id)\n",
    "            for r in f:\n",
    "                row = r.decode('UTF-8')\n",
    "                if(row.startswith('#')):\n",
    "                    continue\n",
    "                rowsplit=row.strip().split('\\t')\n",
    "                abund = None\n",
    "                try:\n",
    "                    abund=float(rowsplit[2])\n",
    "                except:\n",
    "                    pass\n",
    "                ncbi_taxon_id=None\n",
    "                try:\n",
    "                    ncbi_taxon_id=int(rowsplit[1].split('|')[-1])\n",
    "                except:\n",
    "                    pass\n",
    "                d = {\n",
    "                    'clade_string': rowsplit[0],\n",
    "                    'taxonomy_string': rowsplit[1],\n",
    "                    'clade': rowsplit[0].split('|')[-1],\n",
    "                    'relative_abundance': abund,\n",
    "                    'ncbi_taxonomy': ncbi_taxon_id\n",
    "                }\n",
    "                d.update({\n",
    "                    'study_id':study_id,\n",
    "                    'sample_id':sample_id,\n",
    "                    'filetype': filetype\n",
    "                })\n",
    "                res.append(d)\n",
    "        return res\n",
    "\n",
    "#p = beam.Pipeline(InteractiveRunner())\n",
    "\n",
    "gcs = beam.io.gcp.gcsfilesystem.GCSFileSystem({})\n",
    "with beam.Pipeline(options=options) as p:\n",
    "#with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    (p | beam.io.fileio.MatchFiles('gs://cmgd-data/manual/**/metaphlan_bugs_list/*')\n",
    "     | beam.Reshuffle()\n",
    "     | beam.ParDo(CSVFixer())\n",
    "     | beam.Map(lambda x: json.dumps(x))\n",
    "     | beam.io.WriteToText('gs://cmgd-data/temp/metaphlan_bugs_list.json')\n",
    "    )\n",
    "#p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implicitly runs the pipeline and shows the elements in `PCollection` `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'abc.txt'.replace('.txt','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up `PTransform`s that will convert the words to lowercase and then count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_counts = (words\n",
    "                | \"lower\" >> beam.Map(lambda word: word.lower())\n",
    "                | \"lower_count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will return the count using the same words as before but with lowercase.\n",
    "Because all words are converted to lowercase before being counted, some words\n",
    "will have a higher count than before. \n",
    "(e.g. `KING: 2, King: 4, king: 3` will become `king: 9`)\n",
    "Note the parameter `visualize_data=True`. This optional parameter gives you a visualization of the data (see [FAQ #3.How do I read the visualization](../../faq.md#q3)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PCollection[[14]: WriteToText/Write/WriteImpl/FinalizeWrite.None] at 0x7ff8606f7cd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p | beam.io.ReadFromText('gs://cmgd-data/manual/AsnicarF_2017/genefamilies_relab/*.tsv') | beam.io.WriteToText('file:///tmp/abc.txt')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gives you a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that represents the `PCollection` `lower_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PCollection[[12]: WriteToText/Write/WriteImpl/FinalizeWrite.None] at 0x7ff860812390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f | beam.io.WriteToText('/tmp/abc') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the job graph for the pipeline by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.show_graph(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is designed to run easily on a single machine. If you have many such files, add an output sink to your `PCollection` result by doing:\n",
    "```\n",
    "lower_counts | beam.io.WriteToText(<file>)\n",
    "```\n",
    "And if you have millions of such files with billions of words, you need a cluster of machines that have enough processing power and memory to finish processing them in a reasonable amount of time.\n",
    "[Google Cloud Dataflow](https://cloud.google.com/dataflow) takes away the headache of managing such a cluster, parallelizes and reliably runs your Apache Beam pipelines, with intelligent auto-scaling so that you only pay for the resources needed for your pipelines.\n",
    "\n",
    "Refer to [this walkthrough](Dataflow_Word_Count.ipynb) on how to run a Dataflow job using the example code in this notebook.\n",
    "\n",
    "If you have any feedback on this notebook, drop us a line at beam-notebooks-feedback@google.com."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.29.0 for Python 3",
   "language": "python",
   "name": "apache-beam-2.29.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
